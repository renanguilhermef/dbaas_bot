version: "3.9"

services:
  fastapi:
    build:
      context: ./backend
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      - ./backend/oracle_kbs:/app/oracle_kbs
    tty: true
    stdin_open: true

  litellm:
    image: docker.litellm.ai/berriai/litellm:main-stable
    container_name: litellm-proxy
    ports:
      - "4000:4000"
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
      - ./gcloud.json:/app/gcloud.json
    command: [ "--config", "/app/config.yaml", "--detailed_debug" ]

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    ports:
      - "8080:8080"
    environment:
      - OLLAMA_BASE_URL=
      # Note: We use the service name 'litellm' here, not localhost
      - OPENAI_API_BASE_URL=http://litellm-proxy:4000/v1
      - ENABLE_RAG_LOCAL_WEB_FETCH=True
    depends_on:
      - litellm
      - fastapi